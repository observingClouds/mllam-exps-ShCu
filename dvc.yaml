stages:
  mllam_version:
    cmd: source machines/environment.sh; python -c "from neural_lam import __version__; print(__version__)" > version.mllam.txt
  prepare_dataset:
    cmd: python -m mllam_data_prep data/datastore.yaml --output data/datastore.zarr
    deps:
    - data/datastore.yaml
    outs:
    - data/datastore.zarr
  create_graph-slurm:
    cmd: python -m neural_lam.create_graph --config_path data/config.yaml --name 1level
      --levels 1
    params:
    - data/config.yaml:
      - datastore
    deps:
    - data/datastore.zarr
    outs:
    - data/graph/
  train:
    cmd:
    - sbatch -W machines/slurm.train.sh --config_path $PWD/data/config.yaml --logger mlflow
    params:
    - data/training_params.yaml:
      - model
      - graph
      - epochs
      - batch_size
      - hidden_dim
      - processor_layers
      - metrics_watch
      - num_workers
      - num_nodes
      - val_steps_to_log
    deps:
    - version.mllam.txt
    - data/graph/
    - data/config.yaml
    - data/datastore.yaml
    outs:
    - saved_models
  evaluate:
    cmd:
    - sbatch -W --nodes 1 machines/slurm.train.sh --config_path $PWD/data/config.yaml --eval val --load
      $PWD/saved_models/*/last.ckpt --logger mlflow
    params:
    - data/evaluate_params.yaml:
      - model
      - graph
      - epochs
      - batch_size
      - hidden_dim
      - num_workers
      - processor_layers
      - metrics_watch
      - val_steps_to_log
      - num_nodes
      - ar_steps_eval
    deps:
    - version.mllam.txt
    - data/graph/
    - data/config.yaml
    - data/datastore.yaml
    - saved_models
